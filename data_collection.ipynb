{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6rvb/J/AZTnGHeIV5jSHb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pakapakk/cybernews-rss-reader/blob/main/data_collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RSS-Finder"
      ],
      "metadata": {
        "id": "gDYxbI-kvt1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install html5lib requests beautifulsoup4 pytz pandas dateutil feedparser"
      ],
      "metadata": {
        "id": "sdQt7_dSv0zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install feedparser"
      ],
      "metadata": {
        "id": "xWwLpHL1-JxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: REMOVE HTML TAGS FROM THE SCRAPPED DATA // Done\n",
        "# TODO: SAVE TO JSON\n",
        "# TODO: CSV <-> JSON CONVERTER"
      ],
      "metadata": {
        "id": "464EIrKZy5W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJpNeZAuu-Xx"
      },
      "outputs": [],
      "source": [
        "import urllib.parse\n",
        "import html5lib\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_feeds(urls):\n",
        "    custom_feeds = {\n",
        "        'https://thehackernews.com/' : 'https://feeds.feedburner.com/TheHackersNews',\n",
        "        'https://www.bleepingcomputer.com/' : 'https://www.bleepingcomputer.com/feed/',\n",
        "        'https://www.securityweek.com/': 'https://www.securityweek.com/feed/',  #  xml exits but keyword is not in the headers\n",
        "        'https://www.microsoft.com/en-us/security/blog/': 'https://www.microsoft.com/en-us/security/blog/feed/',  #  xml exits but keyword is not in the headers\n",
        "        'https://www.databreachtoday.com/': 'https://www.databreachtoday.com/rss-feeds',  #  xml exits but its only the path, not full url\n",
        "        'https://packetstormsecurity.com/files/': 'https://rss.packetstormsecurity.com/files/',  #\n",
        "        'https://packetstormsecurity.com/files/tags/exploit/': 'https://rss.packetstormsecurity.com/files/tags/exploit/',  #\n",
        "        # 'https://edition.cnn.com/business/tech': 'http://rss.cnn.com/rss/cnn_tech.rss',  #  xml exits but its not on the page source\n",
        "    }\n",
        "\n",
        "    feeds = {}\n",
        "\n",
        "    for url in urls:\n",
        "        if url in custom_feeds:\n",
        "            feeds[url] = custom_feeds[url]\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            html = response.text\n",
        "            tree = html5lib.parse(html, namespaceHTMLElements=False)\n",
        "\n",
        "            base = tree.findall('.//base')\n",
        "            base_url = base[0].attrib['href'] if base and 'href' in base[0].attrib else url\n",
        "\n",
        "            links = tree.findall(\".//head/link[@rel='alternate'][@type='application/atom+xml']\") + \\\n",
        "                    tree.findall(\".//head/link[@rel='alternate'][@type='application/rss+xml']\")\n",
        "\n",
        "            for link in links:\n",
        "                href = link.attrib.get('href', '').strip()\n",
        "                if href:\n",
        "                    feed_url = urllib.parse.urljoin(base_url, href)\n",
        "                    feeds[url] = feed_url\n",
        "                    break\n",
        "\n",
        "            if url not in feeds:\n",
        "                for suffix in [\n",
        "                    'feed', 'feed/', 'rss', 'atom', 'feed.xml',\n",
        "                    '/feed', '/feed/', '/rss', '/atom', '/feed.xml',\n",
        "                    'index.atom', 'index.rss', 'index.xml', 'atom.xml', 'rss.xml',\n",
        "                    '/index.atom', '/index.rss', '/index.xml', '/atom.xml', '/rss.xml',\n",
        "                    '.rss', '/.rss', '?rss=1', '?feed=rss2',\n",
        "                ]:\n",
        "                    try:\n",
        "                        potential_feed = urllib.parse.urljoin(base_url, suffix)\n",
        "                        response = requests.get(potential_feed)\n",
        "                        # if response.status_code == 200 and ('xml' in response.headers.get('Content-Type', '') or 'rss' in response.headers.get('Content-Type', '')):\n",
        "                        if ('xml' in response.headers.get('Content-Type', '') or 'rss' in response.headers.get('Content-Type', '')):\n",
        "                            feeds[url] = potential_feed\n",
        "                            break\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred with URL {url}: {e}\")\n",
        "            feeds[url] = None\n",
        "\n",
        "    return feeds"
      ],
      "metadata": {
        "id": "ACx5l20zgTBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_list = [\n",
        "    'https://www.bleepingcomputer.com/',\n",
        "    'https://thehackernews.com/',\n",
        "    'https://www.theregister.com//security',\n",
        "    'https://www.darkreading.com/',\n",
        "    'https://www.zdnet.com//topic/security',\n",
        "    'https://www.securityweek.com/',\n",
        "    'https://securityaffairs.com/',\n",
        "    # 'https://packetstormsecurity.com/files/',\n",
        "    # 'https://packetstormsecurity.com/files/tags/exploit/',\n",
        "    # 'https://www.databreachtoday.com/',\n",
        "    'https://cyberscoop.com/',\n",
        "    'https://cyberscoop.com/news/threats/cybercrime/',\n",
        "    'https://www.microsoft.com/en-us/security/blog/',\n",
        "    ]"
      ],
      "metadata": {
        "id": "fuDCwI5eJ5Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(url_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1YaV0LerR03",
        "outputId": "f7ad876c-af09-46c1-e6fc-2e43d7c908f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_feeds(url_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVuLTLUKwBNJ",
        "outputId": "7fc5463d-d199-4b57-a59f-d1a5e56ae160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'https://www.bleepingcomputer.com/': 'https://www.bleepingcomputer.com/feed/',\n",
              " 'https://thehackernews.com/': 'https://feeds.feedburner.com/TheHackersNews',\n",
              " 'https://www.theregister.com//security': 'https://www.theregister.com/headlines.atom',\n",
              " 'https://www.darkreading.com/': 'https://www.darkreading.com/rss.xml',\n",
              " 'https://www.zdnet.com//topic/security': 'https://www.zdnet.com/rss.xml',\n",
              " 'https://www.securityweek.com/': 'https://www.securityweek.com/feed/',\n",
              " 'https://securityaffairs.com/': 'https://securityaffairs.com/feed',\n",
              " 'https://cyberscoop.com/': 'https://cyberscoop.com/feed/',\n",
              " 'https://cyberscoop.com/news/threats/cybercrime/': 'https://cyberscoop.com/feed/',\n",
              " 'https://www.microsoft.com/en-us/security/blog/': 'https://www.microsoft.com/en-us/security/blog/feed/'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rss_list = [\n",
        "    'https://www.bleepingcomputer.com/feed/',\n",
        "    'https://feeds.feedburner.com/TheHackersNews',\n",
        "    'https://www.theregister.com/headlines.atom',\n",
        "    'https://www.darkreading.com/rss.xml',\n",
        "    'https://www.zdnet.com/rss.xml',\n",
        "    'https://www.securityweek.com/feed/',\n",
        "    'https://securityaffairs.com/feed/',\n",
        "    'https://cyberscoop.com/feed/',\n",
        "    'https://www.microsoft.com/en-us/security/blog/feed/',\n",
        "    'https://1stsecuritynews.com/feed/',\n",
        "    'https://adamlevin.com/feed/',\n",
        "    'https://www.alstonprivacy.com/feed/',\n",
        "    'https://www.amazingsupport.co.uk/feed/',\n",
        "    'https://www.andrewhay.ca/blog/feed',\n",
        "    'https://architectsecurity.org/feed/',\n",
        "    'https://blog.avast.com/feed',\n",
        "    'https://medium.com/feed/@avi-lumelsky',\n",
        "    'https://aws.amazon.com/blogs/security/feed/',\n",
        "    'https://badcyber.com/feed/',\n",
        "    'https://www.benthamsgaze.org/feed/',\n",
        "    'https://www.blackhillsinfosec.com/blog/feed/',\n",
        "    'https://blog.cadre.net/rss.xml',\n",
        "    'https://www.canadiansecuritymag.com/feed/',\n",
        "    'https://johnmasserini.com/feed/',\n",
        "    'https://blogs.cisco.com/security/feed',\n",
        "    'https://cloudsecurityalliance.org/feed',\n",
        "    'https://www.cyberdefensemagazine.com/feed/',\n",
        "    'http://cyber.uk/feed/',\n",
        "    'https://defensivesecurity.org/feed/',\n",
        "    'https://www.flyingpenguin.com/?feed=rss2',\n",
        "    'https://feeds.feedblitz.com/GDataSecurityBlog-EN&x=1',\n",
        "    'https://gbhackers.com/feed/',\n",
        "    'http://feeds.feedburner.com/GoogleOnlineSecurityBlog',\n",
        "    'https://www.grahamcluley.com/feed/',\n",
        "    'https://www.hackingarticles.in/feed/',\n",
        "    'https://www.hacknos.com/feed/',\n",
        "    'https://medium.com/feed/@hpatton',\n",
        "    'https://blogs.infoblox.com/feed/',\n",
        "    'https://informationsecuritybuzz.com/feed/',\n",
        "    'https://medium.com/feed/@infosecsherpa',\n",
        "    'https://jeffsoh.blogspot.com/feeds/posts/default?alt=rss',\n",
        "    'https://zeltser.com/blog/feed/',\n",
        "    'http://luciusonsecurity.blogspot.com/feeds/posts/default',\n",
        "    'https://marcoramilli.com/feed/',\n",
        "    'https://medium.com/feed/@martinconnarty',\n",
        "    'http://360tek.blogspot.com/feeds/posts/default?alt=rss',\n",
        "    'http://360tek.blogspot.com/feeds/posts/default',\n",
        "    'https://medium.com/feed/@nairuzabulhul',\n",
        "    'http://feeds.feedburner.com/NoticeBored',\n",
        "    'https://www.pindrop.com/feed/',\n",
        "    'https://blog.qualys.com/feed',\n",
        "    'https://rethinksecurity.io/posts/index.xml',\n",
        "    'https://robert.penz.name/feed/',\n",
        "    'http://feeds2.feedburner.com/RogersInfosecBlog',\n",
        "    'https://medium.com/feed/@sai.51192',\n",
        "    'https://secdevil.com/feed/',\n",
        "    'https://www.securion.io/feed',\n",
        "    'https://securityboulevard.com/feed/',\n",
        "    'https://www.sentinelone.com/feed/',\n",
        "    'https://taosecurity.blogspot.com/feeds/posts/default?alt=rss',\n",
        "    'https://www.tcdi.com/blog/feed/',\n",
        "    'http://tech-wreckblog.blogspot.com/feeds/posts/default?alt=rss',\n",
        "    'https://848.co/feed/',\n",
        "    'https://the-infosec.com/feed/',\n",
        "    'https://blog.trailofbits.com/feed/',\n",
        "    'http://feeds.trendmicro.com/TrendMicroResearch',\n",
        "    'https://www.tripwire.com/state-of-security/feed/',\n",
        "    'https://www.troyhunt.com/rss/',\n",
        "    'http://www.veracode.com/blog/feed/',\n",
        "    'https://www.vircom.com/feed/',\n",
        "    'https://www.wikidsystems.com/blog/feeds/rss/',\n",
        "    'https://www.winmagic.com/blog/feed/',\n",
        "    'https://www.wisporg.com/blog-posts?format=RSS',\n",
        "    'https://blog.rootshell.be/feed/',\n",
        "]"
      ],
      "metadata": {
        "id": "OAHAC8blwyWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(rss_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UawcHkTOKTnx",
        "outputId": "901bda15-7c9f-4136-becb-c4335c1bea71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Aggregator\n"
      ],
      "metadata": {
        "id": "KA6Hl9omwNxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(html_text):\n",
        "    \"\"\"Removes HTML tags and returns plain text.\"\"\"\n",
        "    return BeautifulSoup(html_text, 'html.parser').get_text() if html_text else \"\"\n",
        "\n",
        "def get_field(entry, field, default=\"\"):\n",
        "    \"\"\"Retrieves a field from an entry, returns a default if field is missing.\"\"\"\n",
        "    return getattr(entry, field, default) if hasattr(entry, field) else default\n",
        "\n",
        "def format_date_to_custom(date_str, timezone=\"Etc/GMT-7\"):\n",
        "    \"\"\"Formats the date to 'Tue, 05 Nov 2024 11:03:00 +0700' format in GMT+7.\"\"\"\n",
        "    try:\n",
        "        date_obj = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        # Convert to GMT+7 timezone\n",
        "        target_timezone = pytz.timezone(timezone)\n",
        "        date_obj_target = date_obj.replace(tzinfo=pytz.UTC).astimezone(target_timezone)\n",
        "        return date_obj_target.strftime(\"%a, %d %b %Y %H:%M:%S %z\")\n",
        "    except ValueError:\n",
        "        return date_str"
      ],
      "metadata": {
        "id": "57yPhQrsGOGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_feed(feed_list):\n",
        "    entries_data = []\n",
        "\n",
        "    # for site, feed_url in feed_list.items():\n",
        "    #     print(f\"Fetching feed for: {site}\")\n",
        "\n",
        "    for feed_url in feed_list:\n",
        "        # print(f\"Fetching feed for: {site}\")\n",
        "\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        if feed.bozo:\n",
        "            print(f\"Failed to parse feed from {feed_url}\")\n",
        "            continue\n",
        "\n",
        "        feed_source = clean_text(get_field(feed.feed, 'title'))\n",
        "        feed_link = get_field(feed.feed, 'link')\n",
        "        feed_description = clean_text(get_field(feed.feed, 'description'))\n",
        "        fetched_timestamp = pd.Timestamp.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "\n",
        "        entries_count = len(feed.entries)\n",
        "        # print(f\"Entries Count for {site}: {entries_count}\\n\")\n",
        "\n",
        "        for entry in feed.entries:\n",
        "            published_date = str(get_field(entry, 'published'))\n",
        "            formatted_date = format_date_to_custom(published_date) if isinstance(published_date, str) and published_date else \"\"\n",
        "\n",
        "            categories = [\n",
        "                clean_text(tag['term']) if isinstance(tag, dict) else clean_text(tag)\n",
        "                for tag in getattr(entry, 'tags', [])\n",
        "                if tag\n",
        "            ]\n",
        "            categories_list = \", \".join(categories)\n",
        "\n",
        "            content_encoded = get_field(entry, 'content', [{}])[0].get('value', '') or get_field(entry, 'description')\n",
        "            content = clean_text(content_encoded) if content_encoded else clean_text(get_field(entry, 'summary',))\n",
        "            entry_link = get_field(entry, 'link')\n",
        "\n",
        "            entry_data = {\n",
        "                \"Title\": clean_text(get_field(entry, 'title')),\n",
        "                \"Content\": content,\n",
        "                \"Source\": feed_source,\n",
        "                \"Category\": categories_list,\n",
        "                \"Entry Published\": formatted_date,\n",
        "                \"Entry Link\": entry_link,\n",
        "                \"Fetch Timestamp\": fetched_timestamp,\n",
        "            }\n",
        "            entries_data.append(entry_data)\n",
        "\n",
        "    df = pd.DataFrame(entries_data)\n",
        "    print(f\"\\nTotal Entries: {len(entries_data)}\\n\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "ELHxYmAEAB7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entries_data = parse_feed(find_feeds(url_list))"
      ],
      "metadata": {
        "id": "2UnoGJlsz_N7",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e4ab7f-3a01-409a-dea9-926597937fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching feed for: https://www.bleepingcomputer.com/\n",
            "Entries Count for https://www.bleepingcomputer.com/: 13\n",
            "\n",
            "Fetching feed for: https://thehackernews.com/\n",
            "Entries Count for https://thehackernews.com/: 50\n",
            "\n",
            "Fetching feed for: https://www.theregister.com//security\n",
            "Entries Count for https://www.theregister.com//security: 50\n",
            "\n",
            "Fetching feed for: https://www.darkreading.com/\n",
            "Entries Count for https://www.darkreading.com/: 50\n",
            "\n",
            "Fetching feed for: https://www.zdnet.com//topic/security\n",
            "Entries Count for https://www.zdnet.com//topic/security: 20\n",
            "\n",
            "Fetching feed for: https://www.securityweek.com/\n",
            "Entries Count for https://www.securityweek.com/: 11\n",
            "\n",
            "Fetching feed for: https://securityaffairs.com/\n",
            "Entries Count for https://securityaffairs.com/: 10\n",
            "\n",
            "Fetching feed for: https://cyberscoop.com/\n",
            "Entries Count for https://cyberscoop.com/: 10\n",
            "\n",
            "Fetching feed for: https://cyberscoop.com/news/threats/cybercrime/\n",
            "Entries Count for https://cyberscoop.com/news/threats/cybercrime/: 10\n",
            "\n",
            "Fetching feed for: https://www.microsoft.com/en-us/security/blog/\n",
            "Entries Count for https://www.microsoft.com/en-us/security/blog/: 10\n",
            "\n",
            "\n",
            "Total Entries: 234\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entries_data = parse_feed(rss_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czPb7sn3F9tC",
        "outputId": "ed263b4f-f457-438e-bf13-a89901ce74e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to parse feed from https://www.cyberdefensemagazine.com/feed/\n",
            "Failed to parse feed from https://www.hacknos.com/feed/\n",
            "Failed to parse feed from https://rethinksecurity.io/posts/index.xml\n",
            "Failed to parse feed from https://www.winmagic.com/blog/feed/\n",
            "\n",
            "Total Entries: 2292\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save"
      ],
      "metadata": {
        "id": "Bvxou3Fh0rxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KWuToobAhWD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41e7282-5537-43a7-fa9f-4e6573e35b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def save_to_csv(df, filepath='/content/drive/My Drive/news_data/'):\n",
        "    \"\"\"Saves the DataFrame to a CSV file, handling potential duplicates.\"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No new data to save. Skipping save operation.\")\n",
        "        return\n",
        "\n",
        "    csv_filepath = os.path.join(filepath, 'feed_input_demo.csv')\n",
        "\n",
        "    if os.path.exists(csv_filepath):\n",
        "        existing_df = pd.read_csv(csv_filepath)\n",
        "        combined_df = pd.concat([existing_df, df]).drop_duplicates(subset=['Title', 'Entry Link'], keep='first')\n",
        "\n",
        "        # Clear the file and save the updated DataFrame\n",
        "        with open(csv_filepath, 'w+', encoding='utf-8') as f:\n",
        "            f.truncate(0)  # Clear the file contents\n",
        "        print(f\"Added Entries: {len(combined_df) - len(existing_df)}\")\n",
        "        combined_df.to_csv(csv_filepath, index=False, encoding='utf-8', header=True)\n",
        "    else:\n",
        "        # Save the new DataFrame if the file doesn't exist\n",
        "        df.to_csv(csv_filepath, index=False, encoding='utf-8', header=True)\n",
        "        print(f\"File created and saved with {len(df)} entries.\")\n"
      ],
      "metadata": {
        "id": "Hc91WpfCgQTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_to_csv(entries_data)"
      ],
      "metadata": {
        "id": "t4ctFJ3phJ1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138f79fc-263f-43ed-bfc8-ba722291e862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File created and saved with 2292 entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# entries_data.to_csv('feed_input.csv', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "kmGMQOji7Gxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entries_data.to_json('feed_input.json', orient='records', indent=4)"
      ],
      "metadata": {
        "id": "U3IDoYnKiyZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}